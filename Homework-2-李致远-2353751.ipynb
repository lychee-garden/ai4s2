{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/lychee-garden/ai4s2.git\n",
    "!cp ai4s2/EnergyConsumption_hourly.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b8af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74ba6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Helper functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_data_with_date(data_path, train_end, pred_start, pred_end):\n",
    "    \"\"\"Load training and prediction data from CSV file\"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "    train_data = df[df.index <= pd.to_datetime(train_end)][target_col].values\n",
    "    train_data = train_data[~np.isnan(train_data)]\n",
    "    pred_actual = df[(df.index >= pd.to_datetime(pred_start)) & (df.index <= pd.to_datetime(pred_end))][target_col].values\n",
    "    pred_actual = pred_actual[~np.isnan(pred_actual)]\n",
    "    return train_data, pred_actual\n",
    "\n",
    "def create_sequences(data, seq_len=96, pred_len=24):\n",
    "    \"\"\"Create input sequences and target sequences for LSTM\"\"\"\n",
    "    return np.array([data[i:i+seq_len] for i in range(len(data)-seq_len-pred_len+1)]), \\\n",
    "           np.array([data[i+seq_len:i+seq_len+pred_len] for i in range(len(data)-seq_len-pred_len+1)])\n",
    "\n",
    "def smape_loss(y_true, y_pred):\n",
    "    import tensorflow as tf\n",
    "    eps = tf.keras.backend.epsilon()\n",
    "    return tf.reduce_mean(2.0 * tf.abs(y_true - y_pred) / (tf.abs(y_true) + tf.abs(y_pred) + eps))\n",
    "\n",
    "def iterative_forecast(model, X_init, pred_hours, batch_size=24):\n",
    "    \"\"\"Iteratively forecast future values\"\"\"\n",
    "    predictions, current_seq = [], X_init.copy()\n",
    "    for i in range(0, pred_hours, batch_size):\n",
    "        pred_batch = model.predict(current_seq.reshape(1, current_seq.shape[0], 1), verbose=0)[0]\n",
    "        predictions.append(pred_batch if i + batch_size <= pred_hours else pred_batch[:pred_hours - i])\n",
    "        if i + batch_size <= pred_hours:\n",
    "            current_seq = np.concatenate([current_seq[batch_size:], pred_batch])\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    eps = 1e-8\n",
    "    return np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a32a5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Part 1 & 2 - Model Structure Functions\n",
    "\n",
    "def build_model_a(seq_len=96, pred_len=24):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=(seq_len, 1)), \n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True), \n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'), \n",
    "        Dense(32, activation='relu'), \n",
    "        Dense(pred_len)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_b(seq_len=168, pred_len=24):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=(seq_len, 1)), \n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True), \n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'), \n",
    "        Dense(32, activation='relu'), \n",
    "        Dense(pred_len)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9da6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_path, train_end, seq_len, pred_len, build_model_func, \n",
    "                problem_name='', retrain=False, epochs=100, batch_size=64):\n",
    "    \"\"\"\n",
    "    unified model training function\n",
    "    \n",
    "    Parameters:\n",
    "    - train_end: training data end date (e.g., '2017-12-31')\n",
    "    - seq_len: LSTM sequence length\n",
    "    - pred_len: prediction length\n",
    "    - build_model_func: model building function (build_model_a or build_model_b)\n",
    "    - problem_name: problem identifier name (for file naming)\n",
    "    - retrain: whether to force retrain\n",
    "    \n",
    "    Returns:\n",
    "    - model: trained Keras model\n",
    "    - scaler: trained StandardScaler\n",
    "    - train_scaled: scaled training data\n",
    "    \"\"\"\n",
    "    # load training data\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "    train_data = df[df.index <= pd.to_datetime(train_end)][target_col].values\n",
    "    train_data = train_data[~np.isnan(train_data)]\n",
    "    print(f\"Training data: {len(train_data)} points\")\n",
    "    \n",
    "    # generate model and scaler file path\n",
    "    model_name = 'model_problem_a.weights.h5' if 'A' in problem_name.upper() else 'model_problem_b.weights.h5'\n",
    "    scaler_name = 'scaler_problem_a.pkl' if 'A' in problem_name.upper() else 'scaler_problem_b.pkl'\n",
    "    \n",
    "    # data preprocessing\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    X_train, y_train = create_sequences(train_scaled, seq_len=seq_len, pred_len=pred_len)\n",
    "    X_train_r = np.array(X_train, dtype=np.float32).reshape((len(X_train), seq_len, 1))\n",
    "    y_train_all = np.array(y_train, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Training sequences: {len(X_train)}\")\n",
    "    \n",
    "    try:\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        \n",
    "        # check if the saved model exists\n",
    "        if not retrain and os.path.exists(model_name) and os.path.exists(scaler_name):\n",
    "            print(f\"Loading saved model from {model_name}...\")\n",
    "            model = build_model_func(seq_len, pred_len)\n",
    "            model.compile(optimizer='adam', loss=smape_loss, metrics=['mae'])\n",
    "            model.load_weights(model_name)\n",
    "            with open(scaler_name, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            train_scaled = scaler.transform(train_data.reshape(-1, 1)).flatten()\n",
    "            print(\"Model and scaler loaded successfully!\")\n",
    "            return model, scaler, train_scaled\n",
    "        \n",
    "        # train new model\n",
    "        if retrain:\n",
    "            print(\"Force retraining model...\")\n",
    "        else:\n",
    "            print(\"Training new model...\")\n",
    "        \n",
    "        model = build_model_func(seq_len, pred_len)\n",
    "        model.compile(optimizer='adam', loss=smape_loss, metrics=['mae'])\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(model_name, monitor='loss', save_best_only=True, \n",
    "                                   save_weights_only=True, verbose=1)\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "        \n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        history = model.fit(X_train_r, y_train_all, epochs=epochs, batch_size=batch_size,\n",
    "                           callbacks=[early_stopping, checkpoint], verbose=1)\n",
    "        \n",
    "        # save scaler\n",
    "        with open(scaler_name, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        print(f\"\\n✓ Model saved to {model_name}\")\n",
    "        print(f\"✓ Scaler saved to {scaler_name}\")\n",
    "        print(f\"✓ Training completed!\")\n",
    "        \n",
    "        return model, scaler, train_scaled\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaff62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Part 4 - Model Loading and Testing Functions\n",
    "\n",
    "def load_model_with_weights(model_path, scaler_path, data_path, train_end, seq_len, pred_len, build_model_func):\n",
    "    \"\"\"\n",
    "    直接加载模型权重和scaler\n",
    "    \n",
    "    Parameters:\n",
    "    - model_path: 模型权重文件路径\n",
    "    - scaler_path: scaler文件路径\n",
    "    - data_path: 数据文件路径\n",
    "    - train_end: 训练数据结束日期\n",
    "    - seq_len: 序列长度\n",
    "    - pred_len: 预测长度\n",
    "    - build_model_func: 构建模型的函数\n",
    "    \n",
    "    Returns:\n",
    "    - model: 加载了权重的模型\n",
    "    - scaler: 加载的scaler\n",
    "    - train_scaled: 标准化后的训练数据\n",
    "    \"\"\"\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file {model_path} not found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: Scaler file {scaler_path} not found!\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # 加载scaler\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    # 加载训练数据并标准化\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "    train_data = df[df.index <= pd.to_datetime(train_end)][target_col].values\n",
    "    train_data = train_data[~np.isnan(train_data)]\n",
    "    train_scaled = scaler.transform(train_data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # 构建模型并加载权重\n",
    "    model = build_model_func(seq_len, pred_len)\n",
    "    model.compile(optimizer='adam', loss=smape_loss, metrics=['mae'])\n",
    "    model.load_weights(model_path)\n",
    "    \n",
    "    print(f\"✓ Model loaded from {model_path}\")\n",
    "    print(f\"✓ Scaler loaded from {scaler_path}\")\n",
    "    \n",
    "    return model, scaler, train_scaled\n",
    "\n",
    "\n",
    "def test_model(model, scaler, train_scaled, data_path, pred_start, pred_end, \n",
    "               seq_len, problem_name=''):\n",
    "    \"\"\"\n",
    "    unified model testing function\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained Keras model\n",
    "    - scaler: trained StandardScaler\n",
    "    - train_scaled: scaled training data (last seq_len points will be used)\n",
    "    - data_path: CSV file path\n",
    "    - pred_start: prediction start date\n",
    "    - pred_end: prediction end date\n",
    "    - seq_len: sequence length (must be consistent with training)\n",
    "    - problem_name: problem identifier name\n",
    "    \n",
    "    Returns:\n",
    "    - smape: sMAPE value\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Model: {problem_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if model is None or scaler is None:\n",
    "        print(\"Error: Model or scaler is None. Please train the model first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # load actual values for comparison\n",
    "        df = pd.read_csv(data_path)\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        df.set_index('Datetime', inplace=True)\n",
    "        target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "        pred_actual = df[(df.index >= pd.to_datetime(pred_start)) & \n",
    "                         (df.index <= pd.to_datetime(pred_end))][target_col].values\n",
    "        pred_actual = pred_actual[~np.isnan(pred_actual)]\n",
    "        print(f\"Prediction period: {len(pred_actual)} hours\")\n",
    "        \n",
    "        # generate predictions\n",
    "        print(\"Generating predictions...\")\n",
    "        long_term_pred = iterative_forecast(model, train_scaled[-seq_len:], len(pred_actual))[:len(pred_actual)]\n",
    "        long_term_pred_actual = scaler.inverse_transform(long_term_pred.reshape(-1, 1)).flatten()\n",
    "        pred_actual_vals = scaler.inverse_transform(pred_actual.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        # calculate sMAPE\n",
    "        smape = calculate_smape(pred_actual_vals, long_term_pred_actual)\n",
    "        print(f\"\\nForecasting Results - SMAPE: {smape:.4f}\")\n",
    "        \n",
    "        # plot results\n",
    "        plot_len = min(2000, len(pred_actual_vals))\n",
    "        time_idx = pd.date_range(start=pd.to_datetime(pred_start), periods=plot_len, freq='H')\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.plot(time_idx, pred_actual_vals[:plot_len], label='Actual', linewidth=1, alpha=0.7)\n",
    "        plt.plot(time_idx, long_term_pred_actual[:plot_len], label='Predicted', linewidth=1, linestyle='--', alpha=0.7)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Energy Consumption (MW)')\n",
    "        plt.title(f'{problem_name} (First {plot_len} Hours)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        filename = 'problem_a_forecast.png' if 'A' in problem_name.upper() else 'long_term_forecast.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"✓ Plot saved to {filename}\")\n",
    "        \n",
    "        return smape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4b106bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: 116116 points\n",
      "Training sequences: 115997\n",
      "Training new model...\n",
      "Starting training for 80 epochs...\n",
      "Epoch 1/80\n",
      "\u001b[1m 314/1813\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 100ms/step - loss: 1.3668 - mae: 0.9602"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Problem 1A: train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m model_a, scaler_a, train_scaled_a = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEnergyConsumption_hourly.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_end\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2017-12-31\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpred_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuild_model_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_model_a\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproblem_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mProblem A: Forecasting (2018-01 to 2018-08)\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(data_path, train_end, seq_len, pred_len, build_model_func, problem_name, retrain, epochs, batch_size)\u001b[39m\n\u001b[32m     68\u001b[39m early_stopping = EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m1\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_r\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# save scaler\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(scaler_name, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda3\\envs\\ai4s2\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Problem 1A: train\n",
    "model_a, scaler_a, train_scaled_a = train_model(\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2017-12-31',\n",
    "    seq_len=96,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_a,\n",
    "    problem_name='Problem A: Forecasting (2018-01 to 2018-08)',\n",
    "    retrain=False,\n",
    "    epochs=80,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe93c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1A: test\n",
    "# directly load the weights of model_problem_a.weights.h5 as model_a\n",
    "model_a, scaler_a, train_scaled_a = load_model_with_weights(\n",
    "    model_path='model_problem_a.weights.h5',\n",
    "    scaler_path='scaler_problem_a.pkl',\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2017-12-31',\n",
    "    seq_len=96,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_a\n",
    ")\n",
    "\n",
    "if model_a is not None:\n",
    "    smape_a = test_model(\n",
    "        model=model_a,\n",
    "        scaler=scaler_a,\n",
    "        train_scaled=train_scaled_a,\n",
    "        data_path='EnergyConsumption_hourly.csv',\n",
    "        pred_start='2018-01-01',\n",
    "        pred_end='2018-08-31',\n",
    "        seq_len=96,\n",
    "        problem_name='Problem A: Forecasting (2018-01 to 2018-08)'\n",
    "    )\n",
    "else:\n",
    "    print(\"Failed to load model, cannot test.\")\n",
    "    smape_a = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564698cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1B: train\n",
    "model_b, scaler_b, train_scaled_b = train_model(\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2016-12-31',\n",
    "    seq_len=168,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_b,\n",
    "    problem_name='Problem B: Long-Term Forecasting (2017-01 to 2018-08)',\n",
    "    retrain=False,\n",
    "    epochs=50,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1B: test\n",
    "# directly load the weights of model_problem_b.weights.h5 as model_b\n",
    "model_b, scaler_b, train_scaled_b = load_model_with_weights(\n",
    "    model_path='model_problem_b.weights.h5',\n",
    "    scaler_path='scaler_problem_b.pkl',\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2016-12-31',\n",
    "    seq_len=168,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_b\n",
    ")\n",
    "\n",
    "if model_b is not None:\n",
    "    smape_b = test_model(\n",
    "        model=model_b,\n",
    "        scaler=scaler_b,\n",
    "        train_scaled=train_scaled_b,\n",
    "        data_path='EnergyConsumption_hourly.csv',\n",
    "        pred_start='2017-01-01',\n",
    "        pred_end='2018-08-31',\n",
    "        seq_len=168,\n",
    "        problem_name='Problem B: Long-Term Forecasting (2017-01 to 2018-08)'\n",
    "    )\n",
    "else:\n",
    "    print(\"Failed to load model, cannot test.\")\n",
    "    smape_b = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Summary\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nSummary:\")\n",
    "if smape_a is not None:\n",
    "    print(f\"  Problem A (2018-01 to 2018-08) - SMAPE: {smape_a:.4f}\")\n",
    "if smape_b is not None:\n",
    "    print(f\"  Problem B (2017-01 to 2018-08) - SMAPE: {smape_b:.4f}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Imports\n",
    "import random\n",
    "import math\n",
    "from matplotlib import colors\n",
    "random.seed(126)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Ising Model Simulation Functions\n",
    "def initialize_grid(dim):\n",
    "    return np.random.choice([-1, 1], size=(dim, dim))\n",
    "\n",
    "def energy_change(grid, i, j):\n",
    "    n = grid.shape[0]\n",
    "    left, right = (i - 1) % n, (i + 1) % n\n",
    "    up, down = (j + 1) % n, (j - 1) % n\n",
    "    return 2 * grid[i, j] * (grid[left, j] + grid[right, j] + grid[i, up] + grid[i, down])\n",
    "\n",
    "def spin_flip(grid, T):\n",
    "    n = grid.shape[0]\n",
    "    i, j = random.randint(0, n - 1), random.randint(0, n - 1)\n",
    "    delta_E = energy_change(grid, i, j)\n",
    "    if delta_E < 0 or random.random() < math.exp(-delta_E / T):\n",
    "        grid[i, j] = -grid[i, j]\n",
    "    return grid\n",
    "\n",
    "def ising_simulation(n, T, steps=100):\n",
    "    \"\"\"Simulate 2D Ising model using Metropolis algorithm\"\"\"\n",
    "    grid = initialize_grid(n)\n",
    "    for step in range(steps):\n",
    "        for _ in range(n * n):\n",
    "            grid = spin_flip(grid, T)\n",
    "    return grid\n",
    "\n",
    "def generate_data(size, num_temp, temp_min=1.0, temp_max=3.5, repeat=1, max_iter=None):\n",
    "    \"\"\"Generate training/test data from Ising model simulations\"\"\"\n",
    "    if max_iter is None:\n",
    "        max_iter = size**2\n",
    "    X = np.zeros((num_temp * repeat, size**2))\n",
    "    y_label = np.zeros((num_temp * repeat, 1))\n",
    "    y_temp = np.zeros((num_temp * repeat, 1))\n",
    "    temps = np.linspace(temp_min, temp_max, num=num_temp)\n",
    "    for i in range(repeat):\n",
    "        for j in range(num_temp):\n",
    "            grid = ising_simulation(size, temps[j], max_iter)\n",
    "            X[i*num_temp + j, :] = grid.reshape(1, grid.size)\n",
    "            y_label[i*num_temp + j, :] = (temps[j] > 2.269)\n",
    "            y_temp[i*num_temp + j, :] = temps[j]\n",
    "            print(f\"Generated {i*num_temp + j + 1}/{num_temp * repeat}\", end='\\r')\n",
    "    print()\n",
    "    return X, y_label, y_temp\n",
    "\n",
    "def save_ising_data(X, y_label, y_temp, filepath):\n",
    "    \"\"\"\n",
    "    keep the Ising model generated data in the file\n",
    "    \n",
    "    Parameters:\n",
    "    - X: feature data\n",
    "    - y_label: label data (classification)\n",
    "    - y_temp: temperature data (regression)\n",
    "    - filepath: save file path\n",
    "    \"\"\"\n",
    "    np.savez_compressed(filepath, X=X, y_label=y_label, y_temp=y_temp)\n",
    "    print(f\"✓ Data saved to {filepath}\")\n",
    "\n",
    "def load_ising_data(filepath):\n",
    "    \"\"\"\n",
    "    load the Ising model generated data from the file\n",
    "    \n",
    "    Parameters:\n",
    "    - filepath: data file path\n",
    "    \n",
    "    Returns:\n",
    "    - X, y_label, y_temp: loaded data\n",
    "    \"\"\"\n",
    "    data = np.load(filepath)\n",
    "    X = data['X']\n",
    "    y_label = data['y_label']\n",
    "    y_temp = data['y_temp']\n",
    "    print(f\"✓ Data loaded from {filepath}\")\n",
    "    return X, y_label, y_temp\n",
    "\n",
    "def get_or_generate_data(size, num_temp, temp_min=1.0, temp_max=3.5, repeat=1, \n",
    "                         max_iter=None, data_type='train', force_regenerate=False):\n",
    "    \"\"\"\n",
    "    get or generate the Ising model data (if the file exists, load directly, otherwise generate and save)\n",
    "    \n",
    "    Parameters:\n",
    "    - size: grid size\n",
    "    - num_temp: number of temperature points\n",
    "    - temp_min: minimum temperature\n",
    "    - temp_max: maximum temperature\n",
    "    - repeat: number of repetitions for each temperature\n",
    "    - max_iter: maximum number of iterations\n",
    "    - data_type: data type ('train' or 'test')\n",
    "    - force_regenerate: whether to force regenerate (ignore the existing file)\n",
    "    \n",
    "    Returns:\n",
    "    - X, y_label, y_temp: data\n",
    "    \"\"\"\n",
    "    # generate file name\n",
    "    filename = f'ising_data_{data_type}_size{size}_numtemp{num_temp}_repeat{repeat}_maxiter{max_iter}.npz'\n",
    "    \n",
    "    # if the file exists and not force regenerate, load\n",
    "    if not force_regenerate and os.path.exists(filename):\n",
    "        print(f\"Loading existing {data_type} data from {filename}...\")\n",
    "        return load_ising_data(filename)\n",
    "    \n",
    "    # otherwise generate new data\n",
    "    print(f\"Generating new {data_type} data...\")\n",
    "    X, y_label, y_temp = generate_data(size, num_temp, temp_min, temp_max, repeat, max_iter)\n",
    "    \n",
    "    # save data\n",
    "    save_ising_data(X, y_label, y_temp, filename)\n",
    "    \n",
    "    return X, y_label, y_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b66f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Model Building Functions\n",
    "def build_ising_model(input_dim, task_type='classification'):\n",
    "    \"\"\"Build neural network model (TensorFlow or sklearn)\"\"\"\n",
    "    try:\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense, Dropout\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "            Dropout(0.3), Dense(64, activation='relu'),\n",
    "            Dropout(0.2), Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid' if task_type == 'classification' else None)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001),\n",
    "                     loss='binary_crossentropy' if task_type == 'classification' else 'mean_absolute_error',\n",
    "                     metrics=['accuracy' if task_type == 'classification' else 'mae'])\n",
    "        return model, 'keras'\n",
    "    except ImportError:\n",
    "        from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "        if task_type == 'classification':\n",
    "            return MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=500, \n",
    "                                random_state=126, early_stopping=True), 'sklearn'\n",
    "        else:\n",
    "            return MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=500,\n",
    "                              random_state=126, early_stopping=True), 'sklearn'\n",
    "\n",
    "def plot_model_flowchart(task_name, model_type, save_path):\n",
    "    \"\"\"Create model flowchart\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis('off')\n",
    "    layers = ['Input\\n(625)', 'Dense\\n(128)', 'Dense\\n(64)', 'Dense\\n(32)', \n",
    "              'Output\\n(1)' if model_type == 'regression' else 'Output\\n(0/1)']\n",
    "    y_pos = np.linspace(0.9, 0.1, len(layers))\n",
    "    for i, (layer, y) in enumerate(zip(layers, y_pos)):\n",
    "        rect = plt.Rectangle((0.4 - 0.08, y - 0.04), 0.16, 0.08,\n",
    "                           facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.4, y, layer, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "        if i < len(layers) - 1:\n",
    "            ax.arrow(0.4, y - 0.04, 0, -(y_pos[i] - y_pos[i+1] - 0.08),\n",
    "                    head_width=0.015, head_length=0.015, fc='black', ec='black')\n",
    "    ax.text(0.5, 0.95, f'{task_name} Model Flowchart', ha='center', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def train_and_evaluate(model, model_type, X_train, y_train, X_test, y_test, task_name):\n",
    "    \"\"\"Train model and return predictions\"\"\"\n",
    "    if model_type == 'keras':\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2,\n",
    "                 callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "                 verbose=0)\n",
    "        y_pred = model.predict(X_test, verbose=0)\n",
    "        if task_name == 'classification':\n",
    "            y_pred = (y_pred > 0.5).astype(int).flatten()\n",
    "        else:\n",
    "            y_pred = y_pred.flatten()\n",
    "    else:\n",
    "        model.fit(X_train, y_train.flatten())\n",
    "        y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def plot_results(y_true, y_pred, y_temp_test, task_name, metric_name, save_path):\n",
    "    \"\"\"Plot results vs temperature\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "    unique_temps = np.unique(y_temp_test)\n",
    "    metrics_by_temp, temps_list = [], []\n",
    "    for temp in unique_temps:\n",
    "        mask = (y_temp_test.flatten() == temp)\n",
    "        if np.sum(mask) > 0:\n",
    "            if task_name == 'classification':\n",
    "                metric = accuracy_score(y_true[mask], y_pred[mask])\n",
    "            else:\n",
    "                metric = mean_absolute_error(y_true[mask], y_pred[mask])\n",
    "            metrics_by_temp.append(metric)\n",
    "            temps_list.append(temp)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(temps_list, metrics_by_temp, 'o-', linewidth=2, markersize=8,\n",
    "            color='blue' if task_name == 'classification' else 'green')\n",
    "    plt.axvline(x=2.269, color='r', linestyle='--', linewidth=2, label='Tc = 2.269')\n",
    "    plt.xlabel('Temperature (T)', fontsize=12)\n",
    "    plt.ylabel(metric_name, fontsize=12)\n",
    "    plt.title(f'Task {\"A\" if task_name == \"classification\" else \"B\"}: {metric_name} vs Temperature', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11113b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate or load training data (if the file exists, load directly, otherwise generate and save)\n",
    "X_train, y_label_train, y_temp_train = get_or_generate_data(\n",
    "    size=25, num_temp=51, temp_min=1.0, temp_max=3.5, repeat=20, max_iter=625,\n",
    "    data_type='train', force_regenerate=False\n",
    ")\n",
    "\n",
    "# generate or load test data (if the file exists, load directly, otherwise generate and save)\n",
    "X_test, y_label_test, y_temp_test = get_or_generate_data(\n",
    "    size=25, num_temp=21, temp_min=1.0, temp_max=3.5, repeat=20, max_iter=625,\n",
    "    data_type='test', force_regenerate=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n",
    "print(f\"data has been saved to the current folder, the file name is:\")\n",
    "print(f\"  - ising_data_train_size25_numtemp51_repeat20_maxiter625.npz\")\n",
    "print(f\"  - ising_data_test_size25_numtemp21_repeat20_maxiter625.npz\")\n",
    "print(f\"The data will be loaded automatically next time, no need to regenerate.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fec48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task A: Classification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Task A: Classification\")\n",
    "print(\"=\" * 60)\n",
    "model, model_type = build_ising_model(X_train.shape[1], 'classification')\n",
    "plot_model_flowchart(\"Task A\", \"classification\", \"task_a_model_flowchart.png\")\n",
    "print(\"Training model...\")\n",
    "y_pred = train_and_evaluate(model, model_type, X_train, y_label_train, X_test, y_label_test, 'classification')\n",
    "y_true = y_label_test.flatten().astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Overall Test Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "plot_results(y_true, y_pred, y_temp_test, 'classification', 'Test Accuracy', 'task_a_accuracy_vs_temp.png')\n",
    "print(\"Plot saved to task_a_accuracy_vs_temp.png\")\n",
    "print(\"Observations: Accuracy decreases near critical temperature (Tc = 2.269)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ca3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B: Regression\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Task B: Regression\")\n",
    "print(\"=\" * 60)\n",
    "model, model_type = build_ising_model(X_train.shape[1], 'regression')\n",
    "plot_model_flowchart(\"Task B\", \"regression\", \"task_b_model_flowchart.png\")\n",
    "print(\"Training model...\")\n",
    "y_pred = train_and_evaluate(model, model_type, X_train, y_temp_train, X_test, y_temp_test, 'regression')\n",
    "y_true = y_temp_test.flatten()\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(f\"Overall Test MAE: {mean_absolute_error(y_true, y_pred):.4f}\")\n",
    "plot_results(y_true, y_pred, y_temp_test, 'regression', 'Test MAE', 'task_b_mae_vs_temp.png')\n",
    "print(\"Plot saved to task_b_mae_vs_temp.png\")\n",
    "print(\"Observations: MAE increases near critical temperature (Tc = 2.269)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

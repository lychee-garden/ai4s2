{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda2666",
   "metadata": {},
   "outputs": [],
   "source": [
    "###To skip the training process, you can directly load the model and scaler from the following code.\n",
    "!git clone https://github.com/lychee-garden/ai4s2.git\n",
    "!cp ai4s2/EnergyConsumption_hourly.csv .\n",
    "!cp ai4s2/model_problem_a.weights.h5 .\n",
    "!cp ai4s2/scaler_problem_a.pkl .\n",
    "!cp ai4s2/model_problem_b.weights.h5 .\n",
    "!cp ai4s2/scaler_problem_b.pkl ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763808b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###you can view results here\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "print(f\"问题一模型流程图如下所示\")\n",
    "print(f\"1_model_flowchart.png\")\n",
    "display(Image('ai4s2/1_model_flowchart.png'))\n",
    "\n",
    "print(f\"1.a中，最好的sMAPE值为0.1366。可以看到拟合度小区间内较好，但整体拟合度不高。\")\n",
    "print(f\"这是由于训练以及预测时，上下文窗口仅有96小时，仅预测24小时，导致模型无法捕捉到数据的整体趋势。\")\n",
    "print(f\"1.a预测结果与实际结果的对比图如下所示。\")\n",
    "print(f\"1_a_forecast.png\")\n",
    "display(Image('ai4s2/1_a_forecast.png'))\n",
    "\n",
    "print(f\"1.b中，最好的sMAPE值为0.1271。可以看到拟合度小区间内较好，但整体拟合度不高。\")\n",
    "print(f\"主要有三点原因：\")\n",
    "print(f\"其一是，虽然上下文窗口提升至168小时，但上下文太短，还是无法捕捉到数据的整体趋势。\")\n",
    "print(f\"其二是，从年的尺度看，训练数据很少，极易过拟合，\")\n",
    "print(f\"其三是，真实世界影响数据的因素是多模态的，我们无法仅用时间序列来预测。\")\n",
    "print(f\"1.b预测结果与实际结果的对比图如下所示。\")\n",
    "print(f\"1_b_forecast.png\")\n",
    "display(Image('ai4s2/1_b_forecast.png'))\n",
    "\n",
    "print(f\"问题二a模型流程图如下所示\")\n",
    "print(f\"2_a_model_flowchart.png\")\n",
    "display(Image('ai4s2/2_a_model_flowchart.png'))\n",
    "\n",
    "print(f\"2.a的二分类准确率与温度关系的图像如下所示。\")\n",
    "print(f\"可以看到，在温度远离临界温度时，准确率较高，在临界温度附近，准确率较低。符合预期。\")\n",
    "print(f\"在低于临界温度区，准确率出现折线的原因是，测试集数量较少，无法满足大数定律。\")\n",
    "print(f\"2_a_accuracy_vs_temp.png\")\n",
    "display(Image('ai4s2/2_a_accuracy_vs_temp.png'))\n",
    "\n",
    "print(f\"问题二b模型流程图如下所示\")\n",
    "print(f\"2_b_model_flowchart.png\")\n",
    "display(Image('ai4s2/2_b_model_flowchart.png'))\n",
    "\n",
    "print(f\"问题二b的MAE与温度关系的图像如下所示。\")\n",
    "print(f\"可以看到，温度越高，MAE越大,准确率越低。符合预期。\")\n",
    "print(f\"2_b_mae_vs_temp.png\")\n",
    "display(Image('ai4s2/2_b_mae_vs_temp.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b8af21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ba6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Helper functions\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_data_with_date(data_path, train_end, pred_start, pred_end):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "    train_data = df[df.index <= pd.to_datetime(train_end)][target_col].values\n",
    "    train_data = train_data[~np.isnan(train_data)]\n",
    "    pred_actual = df[(df.index >= pd.to_datetime(pred_start)) & (df.index <= pd.to_datetime(pred_end))][target_col].values\n",
    "    pred_actual = pred_actual[~np.isnan(pred_actual)]\n",
    "    return train_data, pred_actual\n",
    "\n",
    "def create_sequences(data, seq_len=96, pred_len=24):\n",
    "    return np.array([data[i:i+seq_len] for i in range(len(data)-seq_len-pred_len+1)]), \\\n",
    "           np.array([data[i+seq_len:i+seq_len+pred_len] for i in range(len(data)-seq_len-pred_len+1)])\n",
    "\n",
    "def smape_loss(y_true, y_pred):\n",
    "    import tensorflow as tf\n",
    "    eps = tf.keras.backend.epsilon()\n",
    "    return tf.reduce_mean(2.0 * tf.abs(y_true - y_pred) / (tf.abs(y_true) + tf.abs(y_pred) + eps))\n",
    "\n",
    "def iterative_forecast(model, X_init, pred_hours, batch_size=24):\n",
    "    predictions = []\n",
    "    current_seq = X_init.copy()\n",
    "    if len(current_seq.shape) == 1:\n",
    "        current_seq = current_seq.reshape(-1, 1)\n",
    "    total_predicted = 0\n",
    "    while total_predicted < pred_hours:\n",
    "        if len(current_seq.shape) == 2:\n",
    "            input_seq = current_seq.reshape(1, current_seq.shape[0], 1)\n",
    "        else:\n",
    "            input_seq = current_seq.reshape(1, -1, 1)\n",
    "        \n",
    "        pred_batch = model.predict(input_seq, verbose=0)[0]\n",
    "        \n",
    "        if len(pred_batch.shape) > 1:\n",
    "            pred_batch = pred_batch.flatten()\n",
    "        \n",
    "        remaining = pred_hours - total_predicted\n",
    "        if len(pred_batch) > remaining:\n",
    "            pred_batch = pred_batch[:remaining]\n",
    "        \n",
    "        predictions.append(pred_batch)\n",
    "        total_predicted += len(pred_batch)\n",
    "        \n",
    "        if total_predicted < pred_hours:\n",
    "            if len(pred_batch.shape) == 1:\n",
    "                pred_batch = pred_batch.reshape(-1, 1)\n",
    "            current_seq = np.vstack([current_seq[len(pred_batch):], pred_batch])\n",
    "    \n",
    "    result = np.concatenate(predictions)\n",
    "    return result[:pred_hours]\n",
    "\n",
    "def calculate_smape(y_true, y_pred):\n",
    "    eps = 1e-8\n",
    "    return np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred) + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a5b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Model Structure Functions\n",
    "def build_model_a(seq_len=96, pred_len=24):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=(seq_len, 1)), \n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True), \n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'), \n",
    "        Dense(32, activation='relu'), \n",
    "        Dense(pred_len)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def build_model_b(seq_len=168, pred_len=24):\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=True, input_shape=(seq_len, 1)), \n",
    "        Dropout(0.3),\n",
    "        LSTM(128, return_sequences=True), \n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'), \n",
    "        Dense(32, activation='relu'), \n",
    "        Dense(pred_len)\n",
    "    ])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9da6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_path, train_end, seq_len, pred_len, build_model_func, \n",
    "                problem_name='', retrain=False, epochs=100, batch_size=64):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "    train_data = df[df.index <= pd.to_datetime(train_end)][target_col].values\n",
    "    train_data = train_data[~np.isnan(train_data)]\n",
    "    print(f\"Training data: {len(train_data)} points\")\n",
    "\n",
    "    problem_name_upper = problem_name.upper()\n",
    "    if 'PROBLEM A' in problem_name_upper or (problem_name_upper.startswith('A') and 'PROBLEM B' not in problem_name_upper):\n",
    "        model_name = 'model_problem_a.weights.h5'\n",
    "        scaler_name = 'scaler_problem_a.pkl'\n",
    "        problem_type = 'A'\n",
    "    else:\n",
    "        model_name = 'model_problem_b.weights.h5'\n",
    "        scaler_name = 'scaler_problem_b.pkl'\n",
    "        problem_type = 'B'\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_data.reshape(-1, 1)).flatten()\n",
    "    X_train, y_train = create_sequences(train_scaled, seq_len=seq_len, pred_len=pred_len)\n",
    "    X_train_r = np.array(X_train, dtype=np.float32).reshape((len(X_train), seq_len, 1))\n",
    "    y_train_all = np.array(y_train, dtype=np.float32)\n",
    "    \n",
    "    print(f\"Training sequences: {len(X_train)}\")\n",
    "    try:\n",
    "        from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "        \n",
    "        if not retrain and os.path.exists(model_name) and os.path.exists(scaler_name):\n",
    "            print(f\"Loading saved model from {model_name}...\")\n",
    "            print(f\"  (This is the correct model for Problem {problem_type})\")\n",
    "            model = build_model_func(seq_len, pred_len)\n",
    "            model.compile(optimizer='adam', loss=smape_loss, metrics=['mae'])\n",
    "            model.load_weights(model_name)\n",
    "            with open(scaler_name, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "            train_scaled = scaler.transform(train_data.reshape(-1, 1)).flatten()\n",
    "            print(\"Model and scaler loaded successfully!\")\n",
    "            return model, scaler, train_scaled\n",
    "        elif not retrain:\n",
    "            if os.path.exists('model_problem_a.weights.h5') and problem_type == 'B':\n",
    "                print(f\"Note: Found model_problem_a.weights.h5, but this is for Problem B.\")\n",
    "                print(f\"      Will train new model for Problem B: {model_name}\")\n",
    "            elif os.path.exists('model_problem_b.weights.h5') and problem_type == 'A':\n",
    "                print(f\"Note: Found model_problem_b.weights.h5, but this is for Problem A.\")\n",
    "                print(f\"      Will train new model for Problem A: {model_name}\")\n",
    "        \n",
    "        if retrain:\n",
    "            print(\"Force retraining model...\")\n",
    "        else:\n",
    "            print(\"Training new model...\")\n",
    "        \n",
    "        model = build_model_func(seq_len, pred_len)\n",
    "        model.compile(optimizer='adam', loss=smape_loss, metrics=['mae'])\n",
    "        \n",
    "        checkpoint = ModelCheckpoint(model_name, monitor='loss', save_best_only=True, \n",
    "                                   save_weights_only=True, verbose=1)\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "        \n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        history = model.fit(X_train_r, y_train_all, epochs=epochs, batch_size=batch_size,\n",
    "                           callbacks=[early_stopping, checkpoint], verbose=1)\n",
    "        \n",
    "        with open(scaler_name, 'wb') as f:\n",
    "            pickle.dump(scaler, f)\n",
    "        print(f\"Training completed!\") \n",
    "        return model, scaler, train_scaled\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaff62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Model Loading and Testing Functions\n",
    "def load_model_with_weights(model_path, scaler_path, data_path, train_end, seq_len, pred_len, build_model_func):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file {model_path} not found!\")\n",
    "        return None, None, None    \n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: Scaler file {scaler_path} not found!\")\n",
    "        return None, None, None    \n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df.set_index('Datetime', inplace=True)\n",
    "    target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "    train_data = df[df.index <= pd.to_datetime(train_end)][target_col].values\n",
    "    train_data = train_data[~np.isnan(train_data)]\n",
    "    train_scaled = scaler.transform(train_data.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    model = build_model_func(seq_len, pred_len)\n",
    "    model.compile(optimizer='adam', loss=smape_loss, metrics=['mae'])\n",
    "    model.load_weights(model_path)    \n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Scaler loaded from {scaler_path}\")\n",
    "    return model, scaler, train_scaled\n",
    "\n",
    "\n",
    "def test_model(model, scaler, train_scaled, data_path, pred_start, pred_end, \n",
    "               seq_len, problem_name=''): \n",
    "    if model is None or scaler is None:\n",
    "        print(\"Error: Model or scaler is None. Please train the model first.\")\n",
    "        return None    \n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        df.set_index('Datetime', inplace=True)\n",
    "        target_col = [c for c in df.columns if 'MW' in c.upper()][0]\n",
    "        pred_actual = df[(df.index >= pd.to_datetime(pred_start)) & \n",
    "                         (df.index <= pd.to_datetime(pred_end))][target_col].values\n",
    "        pred_actual = pred_actual[~np.isnan(pred_actual)]\n",
    "        print(f\"Prediction period: {len(pred_actual)} hours\")\n",
    "        print(\"Generating predictions...\")\n",
    "        initial_seq = train_scaled[-seq_len:].copy()\n",
    "        long_term_pred = iterative_forecast(model, initial_seq, len(pred_actual))\n",
    "        \n",
    "        if len(long_term_pred) > len(pred_actual):\n",
    "            long_term_pred = long_term_pred[:len(pred_actual)]\n",
    "        elif len(long_term_pred) < len(pred_actual):\n",
    "            print(f\"Warning: Predicted length {len(long_term_pred)} < actual length {len(pred_actual)}\")\n",
    "\n",
    "        long_term_pred_actual = scaler.inverse_transform(long_term_pred.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        pred_actual_vals = pred_actual.copy()\n",
    "        print(f\"Predicted values range: [{long_term_pred_actual.min():.2f}, {long_term_pred_actual.max():.2f}]\")\n",
    "        print(f\"Actual values range: [{pred_actual_vals.min():.2f}, {pred_actual_vals.max():.2f}]\")\n",
    "        \n",
    "        smape = calculate_smape(pred_actual_vals, long_term_pred_actual)\n",
    "        print(f\"\\nForecasting Results - SMAPE: {smape:.4f}\")\n",
    "        \n",
    "        plot_len = min(2000, len(pred_actual_vals))\n",
    "        time_idx = pd.date_range(start=pd.to_datetime(pred_start), periods=plot_len, freq='H')\n",
    "        plt.figure(figsize=(16, 6))\n",
    "        plt.plot(time_idx, pred_actual_vals[:plot_len], label='Actual', linewidth=1, alpha=0.7)\n",
    "        plt.plot(time_idx, long_term_pred_actual[:plot_len], label='Predicted', linewidth=1, linestyle='--', alpha=0.7)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Energy Consumption (MW)')\n",
    "        plt.title(f'{problem_name} (First {plot_len} Hours)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        filename = 'problem_a_forecast.png' if 'A' in problem_name.upper() else 'long_term_forecast.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"Plot saved to {filename}\")\n",
    "        return smape\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b106bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1A: train\n",
    "model_a, scaler_a, train_scaled_a = train_model(\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2017-12-31',\n",
    "    seq_len=96,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_a,\n",
    "    problem_name='Problem A: Forecasting (2018-01 to 2018-08)',\n",
    "    retrain=False,\n",
    "    epochs=80,\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe93c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1A: test\n",
    "# directly load the weights of model_problem_a.weights.h5 as model_a\n",
    "model_a, scaler_a, train_scaled_a = load_model_with_weights(\n",
    "    model_path='model_problem_a.weights.h5',\n",
    "    scaler_path='scaler_problem_a.pkl',\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2017-12-31',\n",
    "    seq_len=96,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_a\n",
    ")\n",
    "\n",
    "if model_a is not None:\n",
    "    smape_a = test_model(\n",
    "        model=model_a,\n",
    "        scaler=scaler_a,\n",
    "        train_scaled=train_scaled_a,\n",
    "        data_path='EnergyConsumption_hourly.csv',\n",
    "        pred_start='2018-01-01',\n",
    "        pred_end='2018-08-31',\n",
    "        seq_len=96,\n",
    "        problem_name='Problem A: Forecasting (2018-01 to 2018-08)'\n",
    "    )\n",
    "else:\n",
    "    print(\"Failed to load model, cannot test.\")\n",
    "    smape_a = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564698cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1B: train\n",
    "model_b, scaler_b, train_scaled_b = train_model(\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2016-12-31',\n",
    "    seq_len=168,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_b,\n",
    "    problem_name='Problem B: Long-Term Forecasting (2017-01 to 2018-08)',\n",
    "    retrain=False,\n",
    "    epochs=50,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1B: test\n",
    "# directly load the weights of model_problem_b.weights.h5 as model_b\n",
    "model_b, scaler_b, train_scaled_b = load_model_with_weights(\n",
    "    model_path='model_problem_b.weights.h5',\n",
    "    scaler_path='scaler_problem_b.pkl',\n",
    "    data_path='EnergyConsumption_hourly.csv',\n",
    "    train_end='2016-12-31',\n",
    "    seq_len=168,\n",
    "    pred_len=24,\n",
    "    build_model_func=build_model_b\n",
    ")\n",
    "\n",
    "if model_b is not None:\n",
    "    smape_b = test_model(\n",
    "        model=model_b,\n",
    "        scaler=scaler_b,\n",
    "        train_scaled=train_scaled_b,\n",
    "        data_path='EnergyConsumption_hourly.csv',\n",
    "        pred_start='2017-01-01',\n",
    "        pred_end='2018-08-31',\n",
    "        seq_len=168,\n",
    "        problem_name='Problem B: Long-Term Forecasting (2017-01 to 2018-08)'\n",
    "    )\n",
    "else:\n",
    "    print(\"Failed to load model, cannot test.\")\n",
    "    smape_b = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Summary\n",
    "print(\"\\n\" + \"=\"*60 + \"\\nSummary:\")\n",
    "if smape_a is not None:\n",
    "    print(f\"  Problem A (2018-01 to 2018-08) - SMAPE: {smape_a:.4f}\")\n",
    "if smape_b is not None:\n",
    "    print(f\"  Problem B (2017-01 to 2018-08) - SMAPE: {smape_b:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Imports\n",
    "import random\n",
    "import math\n",
    "from matplotlib import colors\n",
    "random.seed(126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba0b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Ising Model Simulation Functions\n",
    "def initialize_grid(dim):\n",
    "    return np.random.choice([-1, 1], size=(dim, dim))\n",
    "\n",
    "def energy_change(grid, i, j):\n",
    "    n = grid.shape[0]\n",
    "    left, right = (i - 1) % n, (i + 1) % n\n",
    "    up, down = (j + 1) % n, (j - 1) % n\n",
    "    return 2 * grid[i, j] * (grid[left, j] + grid[right, j] + grid[i, up] + grid[i, down])\n",
    "\n",
    "def spin_flip(grid, T):\n",
    "    n = grid.shape[0]\n",
    "    i, j = random.randint(0, n - 1), random.randint(0, n - 1)\n",
    "    delta_E = energy_change(grid, i, j)\n",
    "    if delta_E < 0 or random.random() < math.exp(-delta_E / T):\n",
    "        grid[i, j] = -grid[i, j]\n",
    "    return grid\n",
    "\n",
    "def ising_simulation(n, T, steps=100):\n",
    "    grid = initialize_grid(n)\n",
    "    for step in range(steps):\n",
    "        for _ in range(n * n):\n",
    "            grid = spin_flip(grid, T)\n",
    "    return grid\n",
    "\n",
    "def generate_data(size, num_temp, temp_min=1.0, temp_max=3.5, repeat=1, max_iter=None):\n",
    "    if max_iter is None:\n",
    "        max_iter = size**2\n",
    "    X = np.zeros((num_temp * repeat, size**2))\n",
    "    y_label = np.zeros((num_temp * repeat, 1))\n",
    "    y_temp = np.zeros((num_temp * repeat, 1))\n",
    "    temps = np.linspace(temp_min, temp_max, num=num_temp)\n",
    "    for i in range(repeat):\n",
    "        for j in range(num_temp):\n",
    "            grid = ising_simulation(size, temps[j], max_iter)\n",
    "            X[i*num_temp + j, :] = grid.reshape(1, grid.size)\n",
    "            y_label[i*num_temp + j, :] = (temps[j] > 2.269)\n",
    "            y_temp[i*num_temp + j, :] = temps[j]\n",
    "            print(f\"Generated {i*num_temp + j + 1}/{num_temp * repeat}\", end='\\r')\n",
    "    print()\n",
    "    return X, y_label, y_temp\n",
    "\n",
    "def save_ising_data(X, y_label, y_temp, filepath):\n",
    "    np.savez_compressed(filepath, X=X, y_label=y_label, y_temp=y_temp)\n",
    "    print(f\"✓ Data saved to {filepath}\")\n",
    "\n",
    "def load_ising_data(filepath):\n",
    "    data = np.load(filepath)\n",
    "    X = data['X']\n",
    "    y_label = data['y_label']\n",
    "    y_temp = data['y_temp']\n",
    "    print(f\"✓ Data loaded from {filepath}\")\n",
    "    return X, y_label, y_temp\n",
    "\n",
    "def get_or_generate_data(size, num_temp, temp_min=1.0, temp_max=3.5, repeat=1, \n",
    "                         max_iter=None, data_type='train', force_regenerate=False):\n",
    "    filename = f'ising_data_{data_type}_size{size}_numtemp{num_temp}_repeat{repeat}_maxiter{max_iter}.npz'\n",
    "    if not force_regenerate and os.path.exists(filename):\n",
    "        print(f\"Loading existing {data_type} data from {filename}...\")\n",
    "        return load_ising_data(filename)\n",
    "    print(f\"Generating new {data_type} data...\")\n",
    "    X, y_label, y_temp = generate_data(size, num_temp, temp_min, temp_max, repeat, max_iter)\n",
    "    save_ising_data(X, y_label, y_temp, filename)\n",
    "    return X, y_label, y_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b66f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2: Model Building Functions\n",
    "def build_ising_model(input_dim, task_type='classification'):\n",
    "    try:\n",
    "        from tensorflow.keras.models import Sequential\n",
    "        from tensorflow.keras.layers import Dense, Dropout\n",
    "        from tensorflow.keras.optimizers import Adam\n",
    "        model = Sequential([\n",
    "            Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "            Dropout(0.3), Dense(64, activation='relu'),\n",
    "            Dropout(0.2), Dense(32, activation='relu'),\n",
    "            Dense(1, activation='sigmoid' if task_type == 'classification' else None)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(0.001),\n",
    "                     loss='binary_crossentropy' if task_type == 'classification' else 'mean_absolute_error',\n",
    "                     metrics=['accuracy' if task_type == 'classification' else 'mae'])\n",
    "        return model, 'keras'\n",
    "    except ImportError:\n",
    "        from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "        if task_type == 'classification':\n",
    "            return MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=500, \n",
    "                                random_state=126, early_stopping=True), 'sklearn'\n",
    "        else:\n",
    "            return MLPRegressor(hidden_layer_sizes=(128, 64, 32), max_iter=500,\n",
    "                              random_state=126, early_stopping=True), 'sklearn'\n",
    "\n",
    "def plot_model_flowchart(task_name, model_type, save_path):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.axis('off')\n",
    "    layers = ['Input\\n(625)', 'Dense\\n(128)', 'Dense\\n(64)', 'Dense\\n(32)', \n",
    "              'Output\\n(1)' if model_type == 'regression' else 'Output\\n(0/1)']\n",
    "    y_pos = np.linspace(0.9, 0.1, len(layers))\n",
    "    for i, (layer, y) in enumerate(zip(layers, y_pos)):\n",
    "        rect = plt.Rectangle((0.4 - 0.08, y - 0.04), 0.16, 0.08,\n",
    "                           facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.4, y, layer, ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "        if i < len(layers) - 1:\n",
    "            ax.arrow(0.4, y - 0.04, 0, -(y_pos[i] - y_pos[i+1] - 0.08),\n",
    "                    head_width=0.015, head_length=0.015, fc='black', ec='black')\n",
    "    ax.text(0.5, 0.95, f'{task_name} Model Flowchart', ha='center', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def train_and_evaluate(model, model_type, X_train, y_train, X_test, y_test, task_name):\n",
    "    if model_type == 'keras':\n",
    "        from tensorflow.keras.callbacks import EarlyStopping\n",
    "        model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2,\n",
    "                 callbacks=[EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)],\n",
    "                 verbose=0)\n",
    "        y_pred = model.predict(X_test, verbose=0)\n",
    "        if task_name == 'classification':\n",
    "            y_pred = (y_pred > 0.5).astype(int).flatten()\n",
    "        else:\n",
    "            y_pred = y_pred.flatten()\n",
    "    else:\n",
    "        model.fit(X_train, y_train.flatten())\n",
    "        y_pred = model.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "def plot_results(y_true, y_pred, y_temp_test, task_name, metric_name, save_path):\n",
    "    from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "    unique_temps = np.unique(y_temp_test)\n",
    "    metrics_by_temp, temps_list = [], []\n",
    "    for temp in unique_temps:\n",
    "        mask = (y_temp_test.flatten() == temp)\n",
    "        if np.sum(mask) > 0:\n",
    "            if task_name == 'classification':\n",
    "                metric = accuracy_score(y_true[mask], y_pred[mask])\n",
    "            else:\n",
    "                metric = mean_absolute_error(y_true[mask], y_pred[mask])\n",
    "            metrics_by_temp.append(metric)\n",
    "            temps_list.append(temp)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(temps_list, metrics_by_temp, 'o-', linewidth=2, markersize=8,\n",
    "            color='blue' if task_name == 'classification' else 'green')\n",
    "    plt.axvline(x=2.269, color='r', linestyle='--', linewidth=2, label='Tc = 2.269')\n",
    "    plt.xlabel('Temperature (T)', fontsize=12)\n",
    "    plt.ylabel(metric_name, fontsize=12)\n",
    "    plt.title(f'Task {\"A\" if task_name == \"classification\" else \"B\"}: {metric_name} vs Temperature', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11113b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_label_train, y_temp_train = get_or_generate_data(\n",
    "    size=25, num_temp=51, temp_min=1.0, temp_max=3.5, repeat=20, max_iter=625,\n",
    "    data_type='train', force_regenerate=False\n",
    ")\n",
    "\n",
    "X_test, y_label_test, y_temp_test = get_or_generate_data(\n",
    "    size=25, num_temp=21, temp_min=1.0, temp_max=3.5, repeat=20, max_iter=625,\n",
    "    data_type='test', force_regenerate=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining: {X_train.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n",
    "print(f\"data has been saved to the current folder, the file name is:\")\n",
    "print(f\"  - ising_data_train_size25_numtemp51_repeat20_maxiter625.npz\")\n",
    "print(f\"  - ising_data_test_size25_numtemp21_repeat20_maxiter625.npz\")\n",
    "print(f\"The data will be loaded automatically next time, no need to regenerate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fec48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task A: Classification\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Task A: Classification\")\n",
    "print(\"=\" * 60)\n",
    "model, model_type = build_ising_model(X_train.shape[1], 'classification')\n",
    "plot_model_flowchart(\"Task A\", \"classification\", \"task_a_model_flowchart.png\")\n",
    "print(\"Training model...\")\n",
    "y_pred = train_and_evaluate(model, model_type, X_train, y_label_train, X_test, y_label_test, 'classification')\n",
    "y_true = y_label_test.flatten().astype(int)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Overall Test Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "plot_results(y_true, y_pred, y_temp_test, 'classification', 'Test Accuracy', 'task_a_accuracy_vs_temp.png')\n",
    "print(\"Plot saved to task_a_accuracy_vs_temp.png\")\n",
    "print(\"Observations: Accuracy decreases near critical temperature (Tc = 2.269)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ca3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B: Regression\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Task B: Regression\")\n",
    "print(\"=\" * 60)\n",
    "model, model_type = build_ising_model(X_train.shape[1], 'regression')\n",
    "plot_model_flowchart(\"Task B\", \"regression\", \"task_b_model_flowchart.png\")\n",
    "print(\"Training model...\")\n",
    "y_pred = train_and_evaluate(model, model_type, X_train, y_temp_train, X_test, y_temp_test, 'regression')\n",
    "y_true = y_temp_test.flatten()\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(f\"Overall Test MAE: {mean_absolute_error(y_true, y_pred):.4f}\")\n",
    "plot_results(y_true, y_pred, y_temp_test, 'regression', 'Test MAE', 'task_b_mae_vs_temp.png')\n",
    "print(\"Plot saved to task_b_mae_vs_temp.png\")\n",
    "print(\"Observations: MAE increases near critical temperature (Tc = 2.269)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
